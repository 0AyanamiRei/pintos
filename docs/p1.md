# Project 1: Threads

## Preliminaries

我将分P1分为4个部分: 

1. **Alarm Clock**
2. **优先级调度**
3. **优先级捐赠机制**
4. **BSD优先级**

由于前面低效的实现, 导致负载均衡的测试无法通过, 也没有办法修改, 所以决定回顾前面的实现, 尽量优化.

## Alarm Clock

### 添加的字段

1. [全局] `sleep_list`: 效仿`all_list`和`ready_list`
2. [Thread] `sleepelem`: 利用`list_entry`, 从`sleep_list`获取到TCB的句柄
3. [Thread] `sleep_time`: 倒计时睡眠的时间, 为0则唤醒

### 代码修改

1. 为新添加的字段初始化, `sleep_list`和其他全局链表一样, 在`thread_init()`中被创建, `sleep_time`则在线程控制块的初始化`init_thread()`中进行;


2. 通过时钟中断, `intr_handler` ->...-> `thread_tick()`, 添加唤醒的机制, **扫描sleep_list, sleep_time自减, 若为0唤醒并从链表中移除**

```c
// thread_tick()
/** 睡眠时长自减, 尝试唤醒 */
struct thread *t;
for(struct list_elem *e = list_begin(&sleep_list); e != list_end(&sleep_list); e = list_next(e)) {
  t = list_entry (e, struct thread, sleepelem);
  ASSERT(is_thread(t) == true);
  ASSERT(t->status == THREAD_BLOCKED);
  ASSERT(t->sleep_time > 0);

  if(--(t->sleep_time) <= 0) {
    list_remove(e);
    thread_unblock(t);
  }
}
```

3. 修改接口`timer_sleep()`, 实现逻辑**赋值sleep_time, 加入睡眠队列, 调用thread_block()阻塞自己让出CPU** (需要从上下文状态和实际效果中抉择`thread_block()`还是`thread_yield()`)
  
```c
void
thread_sleep(int64_t sleep_time) {
  // ASSERT (!intr_context ());
  // ASSERT (intr_get_level () == INTR_OFF);
  struct thread *cur = thread_current();
  cur->sleep_time = sleep_time;
  list_push_back(&sleep_list, &cur->sleepelem);
  thread_block();
}

void
timer_sleep (int64_t ticks) {
  if(ticks <= 0 ){
    return;
  }
  ASSERT (intr_get_level () == INTR_ON);
  intr_disable();
  thread_sleep(ticks);
  intr_set_level(INTR_ON);
}
```

### 补充问题

1. 为什么选择`thread_block()`而不是`thread_yield()`
2. 为什么`timer_sleep()`->`thread_sleep()`要关闭中断?

>> `thread_block()`和`thread_yield()`均要求不在中断上下文中调用, 前者调用前要求中断关闭, 设置线程状态为阻塞才会避免busy waiting, 防止调度器反复从`ready_list`中取出睡眠线程后又加入, 浪费CPU时间.

>> 前面提到`thread_block()`的运行要求非中断上下文且关闭.

### BUG Log

在`thread_create()`, 创建的线程在`init_thread()`中被加入全局队列*all_list*, 若在此之后再对添加字段初始化, 则会出现一个空档期, 即*TCB*已加入*all_list*, 但是用于*Alarm Clock*的字段未初始化, 导致结果随机化, 所以需要将这些字段的初始化, 放在*TCB page*分配和将*TCB*加入*all_list*之间.

### 回顾第一版设计

第一次的实现, 首先我没有采用倒计时, 而是正向计时, 额外占用了**TCB**的内存. 其次, 每次中断我在`all_list`中检查睡眠阻塞的线程, 导致在中断处理中浪费了更多的时间. 同时这一次我减少了重复的条件检查, 在确信自己的实现安全的情况下, 可以只保留最内层函数的条件检查.




## 优先级调度

### 添加的字段

优先级的参数`priority`已在**TCB**中, 所以这里我们不需要添加额外的字段.

### 代码修改

我们维护`ready_list`按优先级升序排列, 每次选择下一个运行的线程时, 只需要弹出链尾即可.

**1** 当我们将线程放入`ready_list`时, 我们选择`list_insert_ordered()`有序插入以此维护`ready_list`的有序性.

```c
bool
thread_cmp_priority (const struct list_elem *a,
                     const struct list_elem *b,
                     void *aux UNUSED)
{
  return list_entry(a, struct thread, elem)->priority <= 
         list_entry(b, struct thread, elem)->priority;
}

//thread_block()
list_insert_ordered (&ready_list, &t->elem,
                    (list_less_func*)&thread_cmp_priority,
                      NULL); 

//thread_yield()
if (cur != idle_thread) {
  list_insert_ordered (&ready_list, &cur->elem,
                      (list_less_func*)&thread_cmp_priority,
                      NULL); 
}
```

**2** 在`thread_create()`中, 我们创建新的线程, 在新线程创建之前, 一定有当前运行的线程优先级是最高的, 所以我们只需要检查`thread_current()`和新创建的线程二者优先级大小来决定是否让出CPU即可, 这里有两种方案实现, 第一个修改`thread_unblock`, 比较当前线程和unblock线程的优先级, 然后考虑调用`thread_yield()`, 这里会出现若干问题:
   1. `thread_unblock()`的运行上下文要求不做要求,**是否开启外中断, 是否在中断上下文均有可能**, 而`thread_yield()`**不允许在中断上下文允许**, 即`ASSERT (!intr_context())`, 二者冲突;
   2. 如果在`thread_unblock()`中让出CPU, 那么在只有*main*和*idle*两个线程的时候, *idle*线程通过`idle()->sema_up()->thread_unblock()->thread_yield()->schedule()....->thread_block()->schedule()`调用了两次`schedule()`, 使得`ready_list`在第二次`schedule()`为空, 造成了*idle*每次寻找下一个运行的线程总是得到*idle*的死锁局面;

这些问题可以被解决, 第一个我们根据是否中断上下文来选择调用`thread_yield()`还是`intr_yield_on_return()`, 第二个我们只需要特判当前线程非*idle*即可;

另外一个方案是在`thread_create()`调用过`thread_unblock()`后单独判断是否需要让出CPU, 添加函数`try_yield()`即可, 

```c
//thread_unblock()
if(t->priority > thread_current()->priority) {
  if(intr_context()) {
    intr_yield_on_return();
  } else {
    if (thread_current()->tid != 2){
      thread_yield();
    }
  }
}

void try_yield(struct thread *t) {
  enum intr_level old_level = intr_disable ();
  if(t->priority > thread_current()->priority) {
    thread_yield();
  }
  intr_set_level (old_level);
}
```

**3**, 按照要求需要修改`thread_set_priority()`, 不过这很简单, 比较自己的新优先级和`ready_list`中最高优先级来抉择是否让出CPU即可.

```c
//thread_set_priority()
thread_current ()->priority = new_priority;
if(!list_empty(&ready_list) &&
    list_entry(list_back(&ready_list), struct thread, elem)
    ->priority > new_priority ) {
  thread_yield();
}
```

**4**, 现在我们需要修改同步机制的唤醒, 在文件`synch.c`中, 这里我选择O(1)插入信号量的等待队列, 而O(n)选择最大优先级弹出, 简单来说就是在`sema_down`中O(1)插入push_back, 在`sema_up`中调用`list_max()`选择优先级最大的弹出.

### 补充问题

1. 为什么`thread_create()`可以调用`try_yield()`

检查`thread_create()`的使用上下文, 从未发现过在中断处理的时候有创建新线程的用法, 所以我们可以调用`try_yield()->thread_yield()`, 除此之外, `thread_create()`还提供了其他保障: `ready_list`一定不为空; `thread_current()`一定是此前最高优先级线程

### BUG Log

前面提到过的, 修改`thread_unblock`, 在里面让出CPU的话会导致*idle*连续两次调用`schedule()`, 而`ready_list`中只存在一个*main*线程, 从而导致死锁, 所以需要特判.


## 优先级捐赠机制 - BUG

先来看我们添加的字段, 再来详细说我的实现

### 添加的字段

1. [Thread] `block_lock`, 记录该线程在等待哪一把锁
2. [Thread] `donate_nums`, 收到捐赠优先级的线程数量
3. [Thread] `first_priority`, 线程最原始自己的优先级
4. [Thread] `temp_priority[MAXLOCKS]`, 用于恢复优先级的, 收到捐赠的时候会记录自己的旧优先级
5. [Thread] `k`, 记录了多少个旧优先级
6. [lock] `index`, 每一把锁

### 原理

直接从`lock_acquire`出发看我的实现, 因为只有这里是最最初开始捐赠优先级的地方, 假设调用`lock_acquire()`的当前线程为A线程.

如果A线程发现请求的锁已被持有, 也就是`lock->holder != NULL`, 那么需要考虑自己的优先级是否高于持锁线程的优先级, 这里我调用了我封装的函数`donate_priority()`;

如果线程A的优先级小于等于持锁线程, 那么直接返回即可, 否则要进行优先级捐赠了, 由于一个线程不一定仅收到其他线程的一次捐赠(即自己会持有多把锁, 每一把锁都可能收到捐赠), 所以我在**TCB**中添加的`temp_priority`是以数组的形式, 我的方案中每把锁上面只会记录一次旧优先级, 但是会反复更改当前值. 

```c
/** 记录持锁线程的旧值, 每把锁仅记录一次 */
if(lock->index == -1){
  lock->index = ++to->k;
  to->temp_priority[lock->index] = to->priority;
  to->donate_nums ++;
}
```

换句话说, 在测试`test_priority_donate_one`中, 优先级31的线程持有锁, 这个时候来了一个32优先级的线程请求锁, 那么我会记录`temp_priority[idx]`=31, `lock.index = idx`, 然后修改持锁线程优先级为32, 一段时间后又来了一个33优先级的线程, 那么我仅仅修改持锁线程优先级为33, 而不修改`temp_priority[idx]`. 下面的事实保证了这样做的正确性:

当低优先级持锁的时候, 不断受到更高优先级线程的捐赠使自己的优先级单调递增, 这些线程阻塞在这把锁上, 在锁的等待队列(`sema->waiters`)中记录, 即持锁线程的优先级$priority_{LockHolder}$不小于任何该锁的等待队列中的线程优先级$priority_{LockWaiter}$, 当持锁线程释放锁时, 不管是按优先级增长的路径减小还是直接恢复到最初的优先级, 调度线程的情况是等价的, 因为按优先级调度为我们保证了与该锁相关的这些线程的执行顺序是有序, 按优先级从大到小执行的. 另外一个角度想, 即使是逐步恢复优先级, 那么顺序一定是先恢复自己的优先级, 再进行调度选择, 假设等待队列有序的话, 只要我们满足放锁后每次调度时, 队头(最高优先级)线程的优先级大于自己的优先级(和加入等待队列时的情况一致)即可.

这是针对一把锁的存旧值解释, 此外我还让`donate_nums++`来记录线程A受到过多少把锁上的捐赠记录, 同时对第一次受到捐赠的情况, 我单独用了`first_priority`来记录最原始的优先级, 这些是对受到捐赠的线程**TCB**的修改, 对等待线程一方, 我使用`block_lock`记录下其等待在这把锁上的事实. 最后把优先级传递给持锁线程.

```c
/** 记录持锁线程最初的优先级 */
if(to->first_priority == -1) {
  to->first_priority = to->priority;
}

/** 被阻塞线程记录阻塞的锁 */
if(from->block_lock == NULL){
  from->block_lock = lock;
}

/** 传递高优先级给持锁线程*/
to->priority = from->priority;
```

我们还需要额外解决的问题是嵌套捐赠, 即线程A本身也将优先级捐赠给某个线程, 由于我们有这样一条路径: `lock->holder->block_lock->holder->...`, 即我们可以通过锁的句柄一路获得嵌套的**TCB**句柄, 所以这里沿着这条路径更新每个线程的当前优先级即可.

```c
/** 如果持锁线程曾捐赠过优先级, 需要递归更新优先级 */
while(to->block_lock != NULL) {
  to->block_lock->holder->priority = from->priority;
  from = to;
  to = to->block_lock->holder;
}
```

现在来解释为什么要记录`donate_nums`和`first_priority`, 这个主要是受到测试数据的限制, 如果阅读过所有的测试代码, 那么你会发现这里的优先级调度是十分严格的调度策略, 我最初的想法是无锁的情况下按较大优先级调度, 有锁等待的情况下, 从能够运行的线程中选择较大优先级, 但是这里实验想要让我们实现的是有锁等待的情况下, 也要严格遵守较大优先级调度.

当我们释放锁的时候, 假设为锁*lock*, 如果`temp_priority[lock->index]`不是已记录旧值的最大值, 那么这把锁等待队列中的所有线程都不允许得到调度, 因为其它锁上面等待的线程有比你这些线程优先级更大的, 要实现这一点, 同时又不得不把这些等待在锁*lock*上的线程加入到`ready_list`, 就必须要让当前运行的线程, 也就是持锁线程优先级高于他们. 回到前面, 所以如果`temp_priority[lock->index]`不是已记录旧值的最大值, 那么我们不恢复自己的优先级, 而是将这些等待的线程加入到`ready_list`, 而如果是已记录旧值的最大值, 那么我们允许按锁的粒度恢复优先级, 也就是允许等待线程得到调度.

---

这样的实现方案(受到了朋友的影响, 后面我决定全权自己设计), 蕴含了一个事实: **记录旧值最大, 等价于锁等待队列优先级最大**, 这显然是错误的, 这一点在原版的测试样例中没有设计, 导致了我取巧的做法, 后续复盘时我添加了一个例子就过不了了:

假设两把锁， `a`和`b`，然后`main`线程(优先级为31)持有它们, 我以优先级指代线程了, `32`请求`a`锁, 那么捐赠给`main`, `33`请求`a`锁, 继续捐赠给`main`, `34`请求`b`锁, 依旧需要捐赠给`main`, **这个时候35请求a**, 捐赠给main, 看一下我们现在的布局:

- `a`锁等待队列 32, 33, 35 记录的旧值: 31
- `b`锁等待队列 34 记录的旧值: 33
- `main`的优先级: 35

此时释放`a`锁, 我们必须看到的是`32`, `33`, `35`加入`ready_list`, 同时允许`35`调度, 即`main`优先级恢复到35以下, 但是不允许`32`, `33`, 以及`a`锁的`34`运行, 所以此时`main`的优先级应该恢复到34. 如果采取的上面取巧的做法, 当我们释放`a`锁的时候, 因为记录旧值31 < 33, 我们还不会修改`main`的优先级, 导致`35`不会被调度.

---

至此我发现正确性上也有问题, 不过我觉得在这种实时控制优先级的系统里, 定义正确的执行顺序各自有不同的观点, 我还是认为在有锁阻塞的情况下, 从能运行的线程中(`ready_list`中)选择优先级最高的来运行是比较符合直观的一个调度顺序.

## 优先级捐赠机制 - 最终实现(仅提供思路)

回顾一下我写的那个例子, 持锁线程`main`释放`a`锁之后, 我们要修改`main`的优先级为34, 其来源于`b`锁等待队列中最高优先级, 但是思考一下, 我们需要维持什么信息.

释放`a`锁的时候, 虽然明面上我们同时要阻塞其他锁上的等待线程, 但是由于其他锁并未调用*release*, 所以那些等待线程不会位于`ready_list`中, 不会参与优先级调度, 为了方便表述, 扩展一下例子

pic

当我们释放锁$lock_i$的时候, 我们需要定义一些情况下的语义(正确性?)

1. $maxPrio(Thread_{i1-im})$ == $max()$, 值唯一: 这种情况下, 我们至少需要让$Prio(Thread_{Jm}) == maxPrio(Thread_{i1-im})$的下标集合*J*中的线程运行, 同时我们还需要关注其他锁上的最高优先级, 换言之, 我认为可以让优先级位于$SecondMaxPrio(Thread_{i1-im})$  ~ $maxPrio(Thread_{i1-im})$
2. $maxPrio(Thread_{i1-im})$最大, 值不唯一
2. $maxPrio(Thread_{i1-im})$非最大 

## Advanced Scheduler

### 3.1

***Exercise 3.1*** 与优先级调度程序一样，高级调度程序也根据优先级选择运行线程。不过，**高级调度器不进行优先级捐赠**。

- 因此，我们建议您在开始高级调度程序的工作之前，先让优先级调度程序正常工作（优先级捐赠除外）, 您必须编写代码，允许我们在Pintos启动时选择调度算法策略。
- 默认情况下，优先级调度器必须处于激活状态，但我们必须能够通过`-mlfqs`内核选项选择**4.4BSD调度器**, 当`pintos_init()`中的`parse_options()`对选项进行解析时，传递该选项会将`threads/thread.h`中声明的`thread_mlfqs`设置为`true`, 启用**4.4BSD调度器**后，线程不再直接控制自己的优先级。
- 在函数调用`thread_create()`中`priority`参数必须被忽略, 以及任何`thread_set_priority()`调用
- 而`thread_get_priority()`则应返回调度程序设置的线程当前优先级。

---

***Hints***

- Double check the implementations of your fixed-point arithmetic routines (and ideally have some unit test for them). (仔细检查定点运算例程的实现（最好对其进行一些单元测试）)
- Some simple mistake in these routines could result in mysterious issues in your scheduler. (这些例程中的一些简单错误可能会导致调度程序出现神秘问题。)
- Efficiency matters a lot for the MLFQS exercise. (效率对`MLFQS`运算非常重要。)
- An inefficient implementation can distort the system. (低效的实现可能会歪曲系统。)
- Read the comment in the test case `mlfqs-load-avg.c`. (请阅读测试用例`mlfqs-load-avg.c`)
- In fact, the inefficiency in your alarm clock implementation can also influence your MLFQS behavior.(事实上，alarm clock的低效实现也会影响 MLFQS 的行为。)
- So double-check if your implementation there can be optimized. (因此，请仔细检查您的实现是否可以优化。)

***BSD***

Thread priority is dynamically determined by the scheduler using a formula given below. However, each thread also has an integer nice value that determines how "nice" the thread should be to other threads.

- A nice of zero does not affect thread priority. (nice的值为0不会影响线程的优先级)
- A positive nice, to the maximum of 20, decreases the priority of a thread and causes it to give up some CPU time it would otherwise receive.(nice的值大于0, 最大是20, 会降低线程的优先级, 使得它放弃一些本可获取的CPU时间)
- On the other hand, a negative nice, to the minimum of -20, tends to take away CPU time from other threads.(反之, nice的值小于0, 最小值是-20, 使得线程从其他线程夺走CPU时间)
- The initial thread starts with a nice value of zero. Other threads start with a nice value inherited from their parent thread. (最初的线程的nice值为0, 其他线程的nice值会沿袭其父线程)


### 为什么通过不了全部测试?

{"mlfqs-load-60", test_mlfqs_load_60},
{"mlfqs-load-avg", test_mlfqs_load_avg}

描述`mlfqs-load-avg`即可: 这个测试启动了60个线程, 每个线程都有一个唯一的编号, 通过启动多个线程并让它们在不同的时间点进行忙等和睡眠, 更全面地测试MLFQS的负载平均数计算

```sh
mlfqs-load-avg
make tests/threads/mlfqs-load-avg.result
pintos -v -k -T 480 --bochs  -- -q -mlfqs run mlfqs-load-avg

docker exec -it pintos bash
make tests/threads/priority-donate-multiple.result
rm tests/threads/priority-donate-one.output
pintos -v -k -T 60 --bochs  -- -q  run priority-donate-one < /dev/null 2> tests/threads/priority-donate-one.errors > tests/threads/priority-donate-one.output
target remote localhost:1234
file kernel.o
```

```sh
FAIL tests/threads/mlfqs-load-avg
Some load average values were missing or differed from those expected by more than 2.5.
  time   actual <-> expected explanation
------ -------- --- -------- ----------------------------------------
     2    24.16 >>> 0.05     Too big, by 21.61.
     4    24.16 >>> 0.16     Too big, by 21.50.
     6    23.79 >>> 0.34     Too big, by 20.95.
     8    23.79 >>> 0.58     Too big, by 20.71.
    10    23.79 >>> 0.87     Too big, by 20.42.
    12    23.79 >>> 1.22     Too big, by 20.07.
    14    23.79 >>> 1.63     Too big, by 19.66.
    16    23.79 >>> 2.09     Too big, by 19.20.
    18    23.79 >>> 2.60     Too big, by 18.69.
    20    23.79 >>> 3.16     Too big, by 18.13.
    22    23.79 >>> 3.76     Too big, by 17.53.
    24    23.79 >>> 4.42     Too big, by 16.87.
    26    23.43 >>> 5.11     Too big, by 15.82.
    28    23.43 >>> 5.85     Too big, by 15.08.
    30    23.43 >>> 6.63     Too big, by 14.30.
    32    23.43 >>> 7.46     Too big, by 13.47.
    34    23.43 >>> 8.32     Too big, by 12.61.
    36    23.43 >>> 9.22     Too big, by 11.71.
    38    23.43 >>> 10.15    Too big, by 10.78.
    40    23.43 >>> 11.12    Too big, by 9.81.
    42    23.43 >>> 12.13    Too big, by 8.80.
    44    23.05 >>> 13.16    Too big, by 7.39.
    46    23.05 >>> 14.23    Too big, by 6.32.
    48    23.05 >>> 15.33    Too big, by 5.22.
    50    23.05 >>> 16.46    Too big, by 4.09.
    52    23.05 >>> 17.62    Too big, by 2.93.
    54    23.05 >>> 18.81    Too big, by 1.74.
    56    23.05 >>> 20.02    Too big, by 0.53.
    58    23.05  =  21.26    
    60    23.05  =  22.52    
    62    23.05  =  23.71    
    64    22.68  =  24.80    
    66    22.68 <<< 25.78    Too small, by 0.60.
    68    22.68 <<< 26.66    Too small, by 1.48.
    70    22.68 <<< 27.45    Too small, by 2.27.
    72    22.68 <<< 28.14    Too small, by 2.96.
    74    22.68 <<< 28.75    Too small, by 3.57.
    76    22.68 <<< 29.27    Too small, by 4.09.
    78    22.68 <<< 29.71    Too small, by 4.53.
    80    22.68 <<< 30.06    Too small, by 4.88.
    82    22.32 <<< 30.34    Too small, by 5.52.
    84    22.32 <<< 30.55    Too small, by 5.73.
    86    22.32 <<< 30.68    Too small, by 5.86.
    88    22.32 <<< 30.74    Too small, by 5.92.
    90    22.32 <<< 30.73    Too small, by 5.91.
    92    22.32 <<< 30.66    Too small, by 5.84.
    94    22.32 <<< 30.52    Too small, by 5.70.
    96    22.32 <<< 30.32    Too small, by 5.50.
    98    22.32 <<< 30.06    Too small, by 5.24.
   100    22.32 <<< 29.74    Too small, by 4.92.
   102    21.97 <<< 29.37    Too small, by 4.90.
   104    21.97 <<< 28.95    Too small, by 4.48.
   106    21.97 <<< 28.47    Too small, by 4.00.
   108    21.97 <<< 27.94    Too small, by 3.47.
   110    21.97 <<< 27.36    Too small, by 2.89.
   112    21.97 <<< 26.74    Too small, by 2.27.
   114    21.97 <<< 26.07    Too small, by 1.60.
   116    21.97 <<< 25.36    Too small, by 0.89.
   118    21.97 <<< 24.60    Too small, by 0.13.
   120    21.62  =  23.81    
   122    21.62  =  23.02    
   124    21.62  =  22.26    
   126    21.26  =  21.52    
   128    20.56  =  20.81    
   130    19.88  =  20.12    
   132    19.22  =  19.46    
   134    18.58  =  18.81    
   136    17.97  =  18.19    
   138    17.38  =  17.59    
   140    16.80  =  17.01    
   142    16.25  =  16.45    
   144    15.71  =  15.90    
   146    15.19  =  15.38    
   148    14.69  =  14.87    
   150    14.20  =  14.38    
   152    13.73  =  13.90    
   154    13.28  =  13.44    
   156    12.84  =  13.00    
   158    12.41  =  12.57    
   160    12.00  =  12.15    
   162    11.61  =  11.75    
   164    11.22  =  11.36    
   166    10.85  =  10.99    
   168    10.49  =  10.62    
   170    10.15  =  10.27    
   172     9.81  =  9.93     
   174     9.49  =  9.61     
   176     9.17  =  9.29     
   178     8.87  =  8.98     
```