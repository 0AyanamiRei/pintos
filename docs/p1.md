# Project 1: Threads

## Preliminaries

我将分P1分为4个部分: 

1. **Alarm Clock**
2. **优先级调度**
3. **优先级捐赠机制**
4. **BSD优先级**

由于前面低效的实现, 导致负载均衡的测试无法通过, 也没有办法修改, 所以决定回顾前面的实现, 尽量优化.

## Alarm Clock

### 添加的字段

1. [全局] `sleep_list`: 效仿`all_list`和`ready_list`
2. [Thread] `sleepelem`: 利用`list_entry`, 从`sleep_list`获取到TCB的句柄
3. [Thread] `sleep_time`: 倒计时睡眠的时间, 为0则唤醒

### 代码修改

1. 为新添加的字段初始化, `sleep_list`和其他全局链表一样, 在`thread_init()`中被创建, `sleep_time`则在线程控制块的初始化`init_thread()`中进行;


2. 通过时钟中断, `intr_handler` ->...-> `thread_tick()`, 添加唤醒的机制, **扫描sleep_list, sleep_time自减, 若为0唤醒并从链表中移除**

```c
// thread_tick()
/** 睡眠时长自减, 尝试唤醒 */
struct thread *t;
for(struct list_elem *e = list_begin(&sleep_list); e != list_end(&sleep_list); e = list_next(e)) {
  t = list_entry (e, struct thread, sleepelem);
  ASSERT(is_thread(t) == true);
  ASSERT(t->status == THREAD_BLOCKED);
  ASSERT(t->sleep_time > 0);

  if(--(t->sleep_time) <= 0) {
    list_remove(e);
    thread_unblock(t);
  }
}
```

3. 修改接口`timer_sleep()`, 实现逻辑**赋值sleep_time, 加入睡眠队列, 调用thread_block()阻塞自己让出CPU** (需要从上下文状态和实际效果中抉择`thread_block()`还是`thread_yield()`)
  
```c
void
thread_sleep(int64_t sleep_time) {
  // ASSERT (!intr_context ());
  // ASSERT (intr_get_level () == INTR_OFF);
  struct thread *cur = thread_current();
  cur->sleep_time = sleep_time;
  list_push_back(&sleep_list, &cur->sleepelem);
  thread_block();
}

void
timer_sleep (int64_t ticks) {
  if(ticks <= 0 ){
    return;
  }
  ASSERT (intr_get_level () == INTR_ON);
  intr_disable();
  thread_sleep(ticks);
  intr_set_level(INTR_ON);
}
```

### 补充问题

1. 为什么选择`thread_block()`而不是`thread_yield()`
2. 为什么`timer_sleep()`->`thread_sleep()`要关闭中断?

>> `thread_block()`和`thread_yield()`均要求不在中断上下文中调用, 前者调用前要求中断关闭, 设置线程状态为阻塞才会避免busy waiting, 防止调度器反复从`ready_list`中取出睡眠线程后又加入, 浪费CPU时间.

>> 前面提到`thread_block()`的运行要求非中断上下文且关闭.

### BUG Log

在`thread_create()`, 创建的线程在`init_thread()`中被加入全局队列*all_list*, 若在此之后再对添加字段初始化, 则会出现一个空档期, 即*TCB*已加入*all_list*, 但是用于*Alarm Clock*的字段未初始化, 导致结果随机化, 所以需要将这些字段的初始化, 放在*TCB page*分配和将*TCB*加入*all_list*之间.

### 回顾第一版设计

第一次的实现, 首先我没有采用倒计时, 而是正向计时, 额外占用了**TCB**的内存. 其次, 每次中断我在`all_list`中检查睡眠阻塞的线程, 导致在中断处理中浪费了更多的时间. 同时这一次我减少了重复的条件检查, 在确信自己的实现安全的情况下, 可以只保留最内层函数的条件检查.




## 优先级调度

### 添加的字段

优先级的参数`priority`已在**TCB**中, 所以这里我们不需要添加额外的字段.

### 代码修改

我们维护`ready_list`按优先级升序排列, 每次选择下一个运行的线程时, 只需要弹出链尾即可.

**1** 当我们将线程放入`ready_list`时, 我们选择`list_insert_ordered()`有序插入以此维护`ready_list`的有序性.

```c
bool
thread_cmp_priority (const struct list_elem *a,
                     const struct list_elem *b,
                     void *aux UNUSED)
{
  return list_entry(a, struct thread, elem)->priority <= 
         list_entry(b, struct thread, elem)->priority;
}

//thread_block()
list_insert_ordered (&ready_list, &t->elem,
                    (list_less_func*)&thread_cmp_priority,
                      NULL); 

//thread_yield()
if (cur != idle_thread) {
  list_insert_ordered (&ready_list, &cur->elem,
                      (list_less_func*)&thread_cmp_priority,
                      NULL); 
}
```

**2** 在`thread_create()`中, 我们创建新的线程, 在新线程创建之前, 一定有当前运行的线程优先级是最高的, 所以我们只需要检查`thread_current()`和新创建的线程二者优先级大小来决定是否让出CPU即可, 这里有两种方案实现, 第一个修改`thread_unblock`, 比较当前线程和unblock线程的优先级, 然后考虑调用`thread_yield()`, 这里会出现若干问题:
   1. `thread_unblock()`的运行上下文要求不做要求,**是否开启外中断, 是否在中断上下文均有可能**, 而`thread_yield()`**不允许在中断上下文允许**, 即`ASSERT (!intr_context())`, 二者冲突;
   2. 如果在`thread_unblock()`中让出CPU, 那么在只有*main*和*idle*两个线程的时候, *idle*线程通过`idle()->sema_up()->thread_unblock()->thread_yield()->schedule()....->thread_block()->schedule()`调用了两次`schedule()`, 使得`ready_list`在第二次`schedule()`为空, 造成了*idle*每次寻找下一个运行的线程总是得到*idle*的死锁局面;

这些问题可以被解决, 第一个我们根据是否中断上下文来选择调用`thread_yield()`还是`intr_yield_on_return()`, 第二个我们只需要特判当前线程非*idle*即可;

另外一个方案是在`thread_create()`调用过`thread_unblock()`后单独判断是否需要让出CPU, 添加函数`try_yield()`即可, 

```c
//thread_unblock()
if(t->priority > thread_current()->priority) {
  if(intr_context()) {
    intr_yield_on_return();
  } else {
    if (thread_current()->tid != 2){
      thread_yield();
    }
  }
}

void try_yield(struct thread *t) {
  enum intr_level old_level = intr_disable ();
  if(t->priority > thread_current()->priority) {
    thread_yield();
  }
  intr_set_level (old_level);
}
```

**3**, 按照要求需要修改`thread_set_priority()`, 不过这很简单, 比较自己的新优先级和`ready_list`中最高优先级来抉择是否让出CPU即可.

```c
//thread_set_priority()
thread_current ()->priority = new_priority;
if(!list_empty(&ready_list) &&
    list_entry(list_back(&ready_list), struct thread, elem)
    ->priority > new_priority ) {
  thread_yield();
}
```

### 补充问题

1. 为什么`thread_create()`可以调用`try_yield()`

检查`thread_create()`的使用上下文, 从未发现过在中断处理的时候有创建新线程的用法, 所以我们可以调用`try_yield()->thread_yield()`, 除此之外, `thread_create()`还提供了其他保障: `ready_list`一定不为空; `thread_current()`一定是此前最高优先级线程

### BUG Log

> **Exercise 2.1**

>> 更改过的函数:

1. `thread_create()`
2. `thread_unblock()`

>> 对于2.1, 最明显的需要改动的地方是`thread_create`, 当一个线程被创建后, 需要检查优先级, 在这之前我们要看一下以下几个函数可能出现的上下文场景:

1. `thread_create()` 就目前的信息来看, 还没有发现在中断处理`intr_handler`以及`handler`中创建线程的情况, 所以可以暂且视为**不会在中断上下文中使用**, 也就是`intr_context()`始终返回false; 是否开启外中断, 这是我们可操控的一个状态, 总之这个函数的上下文场景: **开启/关闭外中断**
2. `thread_unblock()` 这个函数出现的情况很多, 简言之**是否开启外中断, 是否在中断上下文均有可能**
3. `thread_yield()` 这是本实验让出CPU的重要函数, 但由于其不允许在中断上下文中使用, 所以需要考虑在哪儿使用.

因此我第一次修改在`thread_create()`中, 在调用`thread_unblock()`将新线程加入*ready_list*后, 比较当前**调用线程**和**被创建线程**的优先级, 如果需要当前线程让出CPU就调用`thread_yield()`

第二次修改, 我解决了在`thread_unblock()`中无法使用`thread_yield()`的问题, `intr_yield_on_return()`允许我们在中断处理程序结束后让出CPU, 而不是等待时间片耗尽, 这期间解决了一个BUG: 查看`idle()->sema_up()->thread_unblock()->thread_yield()....->thread_block()`, 会发现*idle*线程会调用两次`schedule()`, 如果只有*main*和*idle*线程在运行的情况, 一个线程会在中断中加入一次*ready_list*, 调用两次`schedule()`会出现*ready_list*为空, *idle*每次寻找下一个线程总是选择自己的死锁局面.

最终我选择第二次修改的方案, 一来是当下二者都能保证正确性, 二来是后者会频繁出现在锁, 信号量的唤醒机制里, 而这些地方正是我们需要考虑优先级的地方.

>> 从2.2回来, 第二处修改的地方来自调用链: `lock_release()->sema_up()->list_pop_front()->thread_unblock()`

`sema_up()`从`&sema->waiters`选择头节点唤醒, 没有考虑优先级, 所以这里需要修改一下, 调整为选择优先级最高的来解锁*unblock*即可


---

### 2.2

***Exercise 2.2***: 优先级调度的一个问题是 **优先级倒置**: 

分别考虑高、中、低优先级线程 H、M 和 L, 如果 H 需要等待 L（例如，等待 L 持有的锁），而 M 在就绪列表中，那么 H 将永远得不到 CPU，因为低优先级线程得不到任何 CPU 时间。解决这个问题的部分办法是，当 L 持有锁时，H 将其优先级 **捐献**给 L，然后在 L 释放（从而 H 获得）锁后，再收回优先级。

--- 

***Hints***

- You will need to account for all different situations in which priority donation is required. (您需要考虑需要优先捐赠的所有不同情况。)
- You must implement priority donation for locks. You need not implement priority donation for the other Pintos synchronization constructs.(您必须为锁实现优先级捐献。对于其他 Pintos 同步构造，则无需执行优先级捐献。)
- You do need to implement priority scheduling in all cases.(在所有情况下都需要执行优先级调度。)
- Be sure to handle multiple donations, in which multiple priorities are donated to a single thread.(请务必处理多重捐献，即向单个线程捐献多个优先级。)
- if H is waiting on a lock that M holds and M is waiting on a lock that L holds, then both M and L should be boosted to H's priority. (如果 H 正在等待 M 持有的锁，而 M 正在等待 L 持有的锁，那么 M 和 L 的优先级都应提升到 H 的优先级。)
- If necessary, you may impose a reasonable limit on depth of nested priority donation, such as 8 levels. (如有必要，可以对嵌套优先级捐赠的深度进行合理限制，例如 8 级。)

**Note**: if you support nested priority donation, you need to pass the `priority-donate-nest` and `priority-donate-chain` tests.

---

考虑最简单的例子, ABC三个线程, 优先级升序, 当线程B让cpu给C时, *ready_list*中有`main`, `idle`, `A`, `B`, 在C让出CPU之前要想办法把自己的优先级捐赠给A:

1. 线程C首先要知道锁在谁身上: `struct thread* locker = lock->holder`
2. 修改线程A的优先级为A, C之间的较大值, 现在C让出CPU后, 会选择A抢占
3. 线程A释放锁后, 需要复原自己的优先级, 为此需要记录下一些自己优先级的变化, 

**添加的结构**

考虑所有的情况, 根据场景, 我们需要的信息

1. 根据`test_priority_donate_one`~`test_priority_donate_multiple2`的测试用例, 对给定的锁, 我们

#### DATA STRUCTURES

>B1: Copy here the declaration of each new or changed struct or struct member, global or static variable, typedef, or enumeration.  Identify the purpose of each in 25 words or less.


>B2: Explain the data structure used to track priority donation.
>Use ASCII art to diagram a nested donation.  (Alternately, submit a
>.png file.)



#### ALGORITHMS

>B3: How do you ensure that the highest priority thread waiting for
>a lock, semaphore, or condition variable wakes up first?



>B4: Describe the sequence of events when a call to lock_acquire()
>causes a priority donation.  How is nested donation handled?



>B5: Describe the sequence of events when lock_release() is called
>on a lock that a higher-priority thread is waiting for.



#### SYNCHRONIZATION

>B6: Describe a potential race in thread_set_priority() and explain
>how your implementation avoids it.  Can you use a lock to avoid
>this race?



#### RATIONALE

>B7: Why did you choose this design?  In what ways is it superior to
>another design you considered?



## Advanced Scheduler

### 3.1

***Exercise 3.1*** 与优先级调度程序一样，高级调度程序也根据优先级选择运行线程。不过，**高级调度器不进行优先级捐赠**。

- 因此，我们建议您在开始高级调度程序的工作之前，先让优先级调度程序正常工作（优先级捐赠除外）, 您必须编写代码，允许我们在Pintos启动时选择调度算法策略。
- 默认情况下，优先级调度器必须处于激活状态，但我们必须能够通过`-mlfqs`内核选项选择**4.4BSD调度器**, 当`pintos_init()`中的`parse_options()`对选项进行解析时，传递该选项会将`threads/thread.h`中声明的`thread_mlfqs`设置为`true`, 启用**4.4BSD调度器**后，线程不再直接控制自己的优先级。
- 在函数调用`thread_create()`中`priority`参数必须被忽略, 以及任何`thread_set_priority()`调用
- 而`thread_get_priority()`则应返回调度程序设置的线程当前优先级。

---

***Hints***

- Double check the implementations of your fixed-point arithmetic routines (and ideally have some unit test for them). (仔细检查定点运算例程的实现（最好对其进行一些单元测试）)
- Some simple mistake in these routines could result in mysterious issues in your scheduler. (这些例程中的一些简单错误可能会导致调度程序出现神秘问题。)
- Efficiency matters a lot for the MLFQS exercise. (效率对`MLFQS`运算非常重要。)
- An inefficient implementation can distort the system. (低效的实现可能会歪曲系统。)
- Read the comment in the test case `mlfqs-load-avg.c`. (请阅读测试用例`mlfqs-load-avg.c`)
- In fact, the inefficiency in your alarm clock implementation can also influence your MLFQS behavior.(事实上，alarm clock的低效实现也会影响 MLFQS 的行为。)
- So double-check if your implementation there can be optimized. (因此，请仔细检查您的实现是否可以优化。)

***BSD***

Thread priority is dynamically determined by the scheduler using a formula given below. However, each thread also has an integer nice value that determines how "nice" the thread should be to other threads.

- A nice of zero does not affect thread priority. (nice的值为0不会影响线程的优先级)
- A positive nice, to the maximum of 20, decreases the priority of a thread and causes it to give up some CPU time it would otherwise receive.(nice的值大于0, 最大是20, 会降低线程的优先级, 使得它放弃一些本可获取的CPU时间)
- On the other hand, a negative nice, to the minimum of -20, tends to take away CPU time from other threads.(反之, nice的值小于0, 最小值是-20, 使得线程从其他线程夺走CPU时间)
- The initial thread starts with a nice value of zero. Other threads start with a nice value inherited from their parent thread. (最初的线程的nice值为0, 其他线程的nice值会沿袭其父线程)


### 为什么通过不了全部测试?

{"mlfqs-load-60", test_mlfqs_load_60},
{"mlfqs-load-avg", test_mlfqs_load_avg}

描述`mlfqs-load-avg`即可: 这个测试启动了60个线程, 每个线程都有一个唯一的编号, 通过启动多个线程并让它们在不同的时间点进行忙等和睡眠, 更全面地测试MLFQS的负载平均数计算

```sh
mlfqs-load-avg
make tests/threads/mlfqs-load-avg.result
pintos -v -k -T 480 --bochs  -- -q -mlfqs run mlfqs-load-avg

docker exec -it pintos bash
make tests/threads/priority-donate-multiple.result
rm tests/threads/priority-donate-one.output
pintos -v -k -T 60 --bochs  -- -q  run priority-donate-one < /dev/null 2> tests/threads/priority-donate-one.errors > tests/threads/priority-donate-one.output
target remote localhost:1234
file kernel.o
```

```sh
FAIL tests/threads/mlfqs-load-avg
Some load average values were missing or differed from those expected by more than 2.5.
  time   actual <-> expected explanation
------ -------- --- -------- ----------------------------------------
     2    24.16 >>> 0.05     Too big, by 21.61.
     4    24.16 >>> 0.16     Too big, by 21.50.
     6    23.79 >>> 0.34     Too big, by 20.95.
     8    23.79 >>> 0.58     Too big, by 20.71.
    10    23.79 >>> 0.87     Too big, by 20.42.
    12    23.79 >>> 1.22     Too big, by 20.07.
    14    23.79 >>> 1.63     Too big, by 19.66.
    16    23.79 >>> 2.09     Too big, by 19.20.
    18    23.79 >>> 2.60     Too big, by 18.69.
    20    23.79 >>> 3.16     Too big, by 18.13.
    22    23.79 >>> 3.76     Too big, by 17.53.
    24    23.79 >>> 4.42     Too big, by 16.87.
    26    23.43 >>> 5.11     Too big, by 15.82.
    28    23.43 >>> 5.85     Too big, by 15.08.
    30    23.43 >>> 6.63     Too big, by 14.30.
    32    23.43 >>> 7.46     Too big, by 13.47.
    34    23.43 >>> 8.32     Too big, by 12.61.
    36    23.43 >>> 9.22     Too big, by 11.71.
    38    23.43 >>> 10.15    Too big, by 10.78.
    40    23.43 >>> 11.12    Too big, by 9.81.
    42    23.43 >>> 12.13    Too big, by 8.80.
    44    23.05 >>> 13.16    Too big, by 7.39.
    46    23.05 >>> 14.23    Too big, by 6.32.
    48    23.05 >>> 15.33    Too big, by 5.22.
    50    23.05 >>> 16.46    Too big, by 4.09.
    52    23.05 >>> 17.62    Too big, by 2.93.
    54    23.05 >>> 18.81    Too big, by 1.74.
    56    23.05 >>> 20.02    Too big, by 0.53.
    58    23.05  =  21.26    
    60    23.05  =  22.52    
    62    23.05  =  23.71    
    64    22.68  =  24.80    
    66    22.68 <<< 25.78    Too small, by 0.60.
    68    22.68 <<< 26.66    Too small, by 1.48.
    70    22.68 <<< 27.45    Too small, by 2.27.
    72    22.68 <<< 28.14    Too small, by 2.96.
    74    22.68 <<< 28.75    Too small, by 3.57.
    76    22.68 <<< 29.27    Too small, by 4.09.
    78    22.68 <<< 29.71    Too small, by 4.53.
    80    22.68 <<< 30.06    Too small, by 4.88.
    82    22.32 <<< 30.34    Too small, by 5.52.
    84    22.32 <<< 30.55    Too small, by 5.73.
    86    22.32 <<< 30.68    Too small, by 5.86.
    88    22.32 <<< 30.74    Too small, by 5.92.
    90    22.32 <<< 30.73    Too small, by 5.91.
    92    22.32 <<< 30.66    Too small, by 5.84.
    94    22.32 <<< 30.52    Too small, by 5.70.
    96    22.32 <<< 30.32    Too small, by 5.50.
    98    22.32 <<< 30.06    Too small, by 5.24.
   100    22.32 <<< 29.74    Too small, by 4.92.
   102    21.97 <<< 29.37    Too small, by 4.90.
   104    21.97 <<< 28.95    Too small, by 4.48.
   106    21.97 <<< 28.47    Too small, by 4.00.
   108    21.97 <<< 27.94    Too small, by 3.47.
   110    21.97 <<< 27.36    Too small, by 2.89.
   112    21.97 <<< 26.74    Too small, by 2.27.
   114    21.97 <<< 26.07    Too small, by 1.60.
   116    21.97 <<< 25.36    Too small, by 0.89.
   118    21.97 <<< 24.60    Too small, by 0.13.
   120    21.62  =  23.81    
   122    21.62  =  23.02    
   124    21.62  =  22.26    
   126    21.26  =  21.52    
   128    20.56  =  20.81    
   130    19.88  =  20.12    
   132    19.22  =  19.46    
   134    18.58  =  18.81    
   136    17.97  =  18.19    
   138    17.38  =  17.59    
   140    16.80  =  17.01    
   142    16.25  =  16.45    
   144    15.71  =  15.90    
   146    15.19  =  15.38    
   148    14.69  =  14.87    
   150    14.20  =  14.38    
   152    13.73  =  13.90    
   154    13.28  =  13.44    
   156    12.84  =  13.00    
   158    12.41  =  12.57    
   160    12.00  =  12.15    
   162    11.61  =  11.75    
   164    11.22  =  11.36    
   166    10.85  =  10.99    
   168    10.49  =  10.62    
   170    10.15  =  10.27    
   172     9.81  =  9.93     
   174     9.49  =  9.61     
   176     9.17  =  9.29     
   178     8.87  =  8.98     
```